{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "42303it [00:01, 39716.62it/s]\n",
      "100%|██████████| 42303/42303 [00:00<00:00, 1242014.04it/s]\n",
      "/anaconda3/envs/workspace/lib/python3.7/site-packages/ipykernel_launcher.py:58: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "import nltk\n",
    "import re\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "import Infersent\n",
    "from Infersent.models import InferSent\n",
    "\n",
    "metadata = pd.read_csv(\"../data/movie.metadata.tsv\", sep = '\\t', header = None)\n",
    "metadata.columns = [\"movie_id\",1,\"movie_name\",3,4,5,6,7,\"genre\"]\n",
    "\n",
    "plots = []\n",
    "with open(\"../data/plot_summaries.txt\", 'r') as f:\n",
    "    reader = csv.reader(f, dialect='excel-tab') \n",
    "    for row in tqdm(reader):\n",
    "        plots.append(row)\n",
    "\n",
    "movie_id = []\n",
    "plot = []\n",
    "\n",
    "# extract movie Ids and plot summaries\n",
    "for i in tqdm(plots):\n",
    "    movie_id.append(i[0])\n",
    "    plot.append(i[1])\n",
    "\n",
    "# create dataframe\n",
    "movies = pd.DataFrame({'movie_id': movie_id, 'plot': plot})\n",
    "\n",
    "# change datatype of 'movie_id'\n",
    "metadata['movie_id'] = metadata['movie_id'].astype(str)\n",
    "\n",
    "# merge meta with movies\n",
    "movies = pd.merge(movies, metadata[['movie_id', 'movie_name', 'genre']], on = 'movie_id')\n",
    "\n",
    "genres = [] \n",
    "\n",
    "# extract genres\n",
    "for i in movies['genre']: \n",
    "    genres.append(list(json.loads(i).values())) \n",
    "\n",
    "# add to 'movies' dataframe  \n",
    "movies['genre_new'] = genres\n",
    "\n",
    "movies_new = movies[~(movies['genre_new'].str.len() == 0)]\n",
    "\n",
    "def clean_text(text):\n",
    "    # remove a string like {{plot}}\n",
    "    text = re.sub(\"\\s*{{\\w*}}\\s*\", \"\", text)\n",
    "    # remove backslash-apostrophe \n",
    "    text = re.sub(\"\\'\", \"\", text)\n",
    "    return text\n",
    "\n",
    "movies_new['clean_plot'] = movies_new['plot'].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_vocabulary = []\n",
    "\n",
    "for movie_id in movies_new['movie_id']:\n",
    "    \n",
    "    plot_sr = movies_new[movies_new['movie_id']==movie_id]['clean_plot']\n",
    "    \n",
    "    for str_obj in plot_sr:\n",
    "        plot=str_obj\n",
    "    \n",
    "    sentence_list = sent_tokenize(plot)\n",
    "\n",
    "    new_vocabulary = new_vocabulary + sentence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size : 100000\n",
      "Found 83944(/149475) words with w2v vectors\n",
      "New vocab size : 183947 (added 83944 words)\n"
     ]
    }
   ],
   "source": [
    "V = 2\n",
    "MODEL_PATH = '/Users/soumava/MovieSummaries/InferSent/encoder/infersent%s.pkl' % V\n",
    "params_model = {'bsize': 128, 'word_emb_dim': 300, 'enc_lstm_dim': 2048,\n",
    "                'pool_type': 'max', 'dpout_model': 0.0, 'version': V}\n",
    "model = InferSent(params_model)\n",
    "model.load_state_dict(torch.load(MODEL_PATH))\n",
    "\n",
    "W2V_PATH = '/Users/soumava/MovieSummaries/crawl-300d-2M.vec'\n",
    "model.set_w2v_path(W2V_PATH)\n",
    "\n",
    "model.build_vocab_k_words(K=100000)\n",
    "model.update_vocab(new_vocabulary, tokenize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-fd394f6e858b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0membedding_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0membedding_array\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mnew_vocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0membedding_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmovie_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding_array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "embedding_dict = {}\n",
    "\n",
    "count = 0\n",
    "\n",
    "for movie_id in movies_new['movie_id']:\n",
    "    \n",
    "    plot_sr = movies_new[movies_new['movie_id']==movie_id]['clean_plot']\n",
    "    \n",
    "    for str_obj in plot_sr:\n",
    "        plot=str_obj\n",
    "    \n",
    "    sentence_list = sent_tokenize(plot)\n",
    "\n",
    "    embedding_array = model.encode(sentence_list, tokenize=True)\n",
    "\n",
    "    embedding_dict[movie_id] = embedding_array\n",
    "\n",
    "    count += 1\n",
    "\n",
    "    print(count)\n",
    "\n",
    "    if count%10000==0:\n",
    "        print('PLOTS processed : {}/{}'.format(count, movies_new.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('xyz.npy', embedding_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
